{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Support Vector Machines\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-1accb9db25dfda6ca85d562f37b4e0a4.webp\" width=\"60%\"></p>\n",
    "\n",
    "<hr>\n",
    "<hr>\n",
    "\n",
    "\n",
    "### Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "%matplotlib inline \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, classification_report, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.svm import SVC, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop \n",
    "data.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "\n",
    "X = data.drop('diagnosis', axis=1)\n",
    "y= data['diagnosis']\n",
    "\n",
    "# split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.30, random_state=14)\n",
    "\n",
    "\n",
    "label = LabelEncoder()\n",
    "y_train = label.fit_transform(y_train)\n",
    "y_test = label.transform(y_test)\n",
    "\n",
    "\n",
    "# Normalização \n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Classificação\n",
    "<hr>\n",
    "<br>\n",
    "\n",
    "\n",
    "O SVM padrão toma como entrada um conjunto de dados e prediz, para cada entrada dada, qual de duas possíveis classes a entrada faz parte, o que faz do SVM um classificador linear binário não probabilístico. Dados um conjunto de exemplos de treinamento, cada um marcado como pertencente a uma de duas categorias, um algoritmo de treinamento do SVM constrói um modelo que atribui novos exemplos a uma categoria ou outra. Um modelo SVM é uma representação de exemplos como pontos no espaço, mapeados de maneira que os exemplos de cada categoria sejam divididos por um espaço claro que seja tão amplo quanto possível. Os novos exemplos são então mapeados no mesmo espaço e preditos como pertencentes a uma categoria baseados em qual o lado do espaço eles são colocados.\n",
    "\n",
    "\n",
    "Em outras palavras, o que uma SVM faz é encontrar uma linha de separação, mais comumente chamada de hiperplano entre dados de duas classes. Essa linha busca maximizar a distância entre os pontos mais próximos em relação a cada uma das classes\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Quem são os vetores de suporte? O vetor de suporte é uma amostra incorretamente classificada ou uma amostra próxima a um limite. Olhando para o enredo abaixo. As amostras com círculos vermelhos são exatamente o limite de decisão. No SVM, apenas os vetores de suporte têm um impacto efetivo no treinamento do modelo, ou seja, remover o vetor não de suporte não tem nenhum efeito sobre o modelo. Por quê? Vamos descobrir isso a partir de sua função de custo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*VXVgDMOdz0iigBIu9sMzGA.png\" width=\"60%\"></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função de perda do SVM é muito semelhante à da Regressão Logística. Observando-o por y = 1 e y = 0 separadamente na plotagem abaixo, a linha preta é a função de custo da regressão logística e a linha vermelha é para SVM. Observe que o eixo X aqui é a saída do modelo bruto, θᵀx. Lembre-se de colocar a saída do modelo bruto na Função Sigmoide nos fornece a hipótese da Regressão Logística. Qual é a hipótese para SVM? É simples e direto. Quando θᵀx ≥ 0, preveja 1, caso contrário, preveja 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>\n",
    "<img src=\"https://miro.medium.com/max/2000/1*qfZnRoVp-A0a4jMLwTBL6g.png\" width=\"60%\"></p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hipóstese do SVM é esta: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=center>\n",
    "<img src=\"https://miro.medium.com/max/2000/1*kHe3Mntx7cgFkFfurTIijA.png\" width=\"70%\"></p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Então, de volta ao gráfico da função de perda, também conhecido como <b> Hinge Loss </b>, quando o real é 1 (plot esquerdo como abaixo), se θᵀx ≥ 1, sem custo algum, se θᵀx <1, o custo aumenta à medida que o valor de θᵀx diminui. Esperar! Quando θᵀx ≥ 0, já previmos 1, que é a previsão correta. Por que o custo começa a aumentar de 1 em vez de 0? Sim, o SVM aplica punições tanto às previsões incorretas quanto àquelas próximas ao limite de decisão (0 <θᵀx <1), é assim que as chamamos de <b> vetores de suporte </b>.\n",
    "\n",
    "<br>\n",
    "\n",
    "Quando os pontos de dados estão exatamente na margem, θᵀx = 1, quando os pontos de dados estão entre o limite de decisão e a margem, 0 <θᵀx <1. Explicarei por que alguns pontos de dados aparecem dentro da margem posteriormente. Quanto ao motivo pelo qual a remoção de vetores não suportados não afeta o desempenho do modelo, podemos respondê-lo agora. Lembre-se de que o processo de ajuste do modelo é minimizar a função de custo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=center>\n",
    "<img src=\"https://miro.medium.com/max/2000/1*PpGKA80nNW29SX3Y78Kkzg.png\" width=\"70%\"></p>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos escrever a fórmula para a função de custo do SVM:\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*AndL5FYso8ad7LSrie8zoA.png\" width=\"70%\"></p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também podemos adicionar regularização ao SVM. Por exemplo, adicionando termo regularizado L2 ao SVM, a função de custo foi alterada para:\n",
    "\n",
    "\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://miro.medium.com/max/2000/1*h5QqCdm48pt84bazOPvExA.png\" width=\"70%\"></p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferente da Regressão Logística usando λ como parâmetro na frente do termo regularizado para controlar o peso da regularização, correspondentemente, o SVM usa C na frente do termo de ajuste. Intuitivamente, o termo ajuste enfatiza o ajuste muito bem ao modelo, encontrando coeficientes ótimos, e o termo regularizado controla a complexidade do modelo, restringindo o grande valor dos coeficientes.\n",
    "\n",
    "<br>\n",
    "\n",
    "Há uma troca entre ajustar bem o modelo no conjunto de dados de treinamento e a complexidade do modelo que pode levar ao sobreajuste, que pode ser ajustado ajustando o valor de λ ou C. Ambos λ e C priorizam quanto nos preocupamos em otimizar prazo adequado e prazo regularizado. Colocando em diferentes locais da função de custo, C realmente desempenha um papel semelhante a 1 / λ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> C com valores grandes: Fornece uma hipótese de viés baixo e alta variância (OVERFITTING) </b>\n",
    "\n",
    "<b> C com valores pequenos: Fornece uma hipótese de viés alto e baixa variância (UNDERFITTING) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<p align=center>\n",
    "<img src=\"https://miro.medium.com/max/2000/1*DSHIKH8TiiN1FeTKtfDzrQ.png\" width=\"70%\"></p>\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função de Kernel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em geral SVM tem um bom desempenho e oferece suporte para espaço lineares e não lineares, usando o que chamamos de <b> Kernel Trick </b> onde a idéia é que podemos criar uma fronteira de decisão em uma nova dimensão minimizando uma fórmula que seja mais fácil de calcular, em comparação a realmente mapear os pontos para uma nova dimensão.\n",
    "\n",
    "O kernel default no Scikit-learn é o RBF que é o Radial Basis Function, que é capaz de mapear um espaço de entrada em espaço com mais dimensões.\n",
    "\n",
    "A função <b> svm() </b> aceita os seguintes kernels: radial, linear, polynomial e sigmoide, Diferentes kernels ajustam diferentes modelos e, consequentemente, diferentes valores preditos.\n",
    "\n",
    "\n",
    "* RBF \n",
    "* Linear \n",
    "* Polynomial \n",
    "* Sigmoid "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=center>\n",
    "<img src=\"https://d3i71xaburhd42.cloudfront.net/3a92a26a66efba1849fa95c900114b9d129467ac/3-TableI-1.png\" width=\"70%\"></p>\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Radial Basis Function - RBF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que há dentro da Função Kernel? Em outras palavras, como devemos descrever a proximidade de x aos pontos de referência? Existem tipos diferentes. O Kernel Gaussiano é um dos mais populares. É calculado com a Distância Euclidiana de dois vetores e o parâmetro σ que descreve a suavidade da função. O kernel gaussiano fornece uma boa intuição. Se x ≈ l⁽¹⁾, f1 ≈ 1, se x estiver longe de l⁽¹⁾, f1 ≈ 0. No pacote Scikit-learn SVM, o Gaussian Kernel é mapeado para <b>'rbf', Radial Basis Function Kernel </b> , a única diferença é 'rbf' usa γ para representar 1 / 2σ² de Gauss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=center>\n",
    "<img src=\"https://ww2.mathworks.cn/matlabcentral/mlc-downloads/downloads/4356651d-2606-4b23-b053-60647095485d/dea2832c-c54a-4951-a9a2-0c67fe8457df/images/screenshot.jpg\" width=\"50%\"></p> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       108\n",
      "           1       1.00      0.94      0.97        63\n",
      "\n",
      "    accuracy                           0.98       171\n",
      "   macro avg       0.98      0.97      0.97       171\n",
      "weighted avg       0.98      0.98      0.98       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SVM kernel RBF  \n",
    "\n",
    "mdl = SVC(C=1.0, gamma=0.7, kernel='rbf', random_state=42, probability=True)\n",
    "mdl.fit(X_train, y_train)\n",
    "y_pred = mdl.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atenção para as diferenças encontradas em cada função de kernel, quando estamos usando RBF por exemplo, nós usamos um parâmetro chamado <b> Gamma </b> na função, este parâmetro não é aplicado na função de kernel Polynomial por exemplo, muita atenção quando for alterar e definir os Hiperparâmetros do modelo, pois as formulas se alteram de um Kernel para outro. \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Algumas características: </b>\n",
    "\n",
    "* Em caso de outlier a SVM busca a melhor forma possível de classificação e, se necessário, desconsidera o outlier;\n",
    "\n",
    "* Funciona muito bem em domínios complicados, em que existe uma clara margem de separação;\n",
    "\n",
    "* Não funciona bem em conjuntos de dados muito grandes, pois exige inversão de matriz - aumentando a complexidade computacional com até o cubo do volume de dados;\n",
    "\n",
    "* Não funciona bem em conjunto de dados com grande quantidade de ruídos;\n",
    "\n",
    "* Se as classes estiverem muito sobrepostas deve-se utilizar apenas evidências independentes (devido ao fato de não ser muito bom com dados com muitos ruídos);\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "*  Os SVMs aprendem fronteiras de decisão lineares\n",
    "(como os perceptrons)\n",
    "* Busca o hiperplano que maximiza a margem\n",
    "* O hiperplano ótimo vem a ser uma combinação linear dos\n",
    "vetores de suporte\n",
    "* Transforma problemas não lineares em um espaço\n",
    "de mais alta dimensão usando funções de kernel\n",
    "* Então há uma chance maior de que neste espaço\n",
    "transformado, as classes serão linearmente\n",
    "separáveis.\n",
    "\n",
    "<hr>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências\n",
    "\n",
    "http://www.ppgia.pucpr.br/~alekoe/AM/2014/8-SVM-AM-2014.pdf\n",
    "<br>\n",
    "https://pdfs.semanticscholar.org/8469/81890b81ae5d9ff5cedfcdbd99150a8bde13.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
